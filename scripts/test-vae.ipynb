{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vae.TweetData(debug=True, max_df=0.1)\n",
    "v.get_tweet_count_vecs()\n",
    "x_tr_tensor, x_test_tensor = v.to_tensor_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader to handle the data\n",
    "train_loader = DataLoader(torch.Tensor(v.X_train.todense()), batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(torch.Tensor(v.X_test.todense()), batch_size=128, shuffle=False)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = vae.VAE(vocab=v.get_vocab_size(), num_components=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\t\"\"\"\n",
    "\tShould rename -- PFA for Poisson Factor Analysis\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, vocab, num_components=20, prior_mean=0, prior_var=1):\n",
    "\t\t\"\"\"\n",
    "\t\tInputs\n",
    "\t\t--------\n",
    "\t\tvocab<int>: the size of the vocabulary\n",
    "\n",
    "\t\tThis model only has the variational layer, then the output\n",
    "\t\tto the reconstruction. At this point, there are no hidden layers.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(VAE, self).__init__()\n",
    "\t\tself.num_components = num_components\n",
    "\n",
    "\t\tself.prior_mean = prior_mean\n",
    "\t\tself.prior_var = prior_var\n",
    "\n",
    "\t\tself.enc_mu = nn.Linear(vocab, num_components, bias=False)\n",
    "\t\tself.enc_logvar = nn.Linear(vocab, num_components, bias=False)\n",
    "\t\tself.W_tilde = torch.rand(num_components, vocab)\n",
    "\t\tself.pois_nll = nn.PoissonNLLLoss(log_input=False)\n",
    "\t\tself.softplus = nn.Softplus()\n",
    "\n",
    "\tdef reparameterize(self, mu, logvar):\n",
    "\t\tstd = torch.exp(0.5*logvar)\n",
    "\t\teps = torch.randn_like(std)\n",
    "\t\treturn mu + eps*std\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmu = self.enc_mu(x)\n",
    "\t\tlogvar = self.enc_logvar(x)\n",
    "\n",
    "\t\ts_tilde = self.reparameterize(mu, logvar)\n",
    "\n",
    "\t\ts = self.softplus(s_tilde)\n",
    "\t\tW = self.softplus(self.W_tilde)\n",
    "\n",
    "\t\treturn s, W, mu, logvar\n",
    "\n",
    "\tdef get_topic_dist(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tWhen it comes to looking at the norm, we want to calculate the \n",
    "\t\tprobability that a certain sample belongs to each topic.\n",
    "\t\t\"\"\"\n",
    "\t\ts, _ = self.encode(x)\n",
    "\t\tW = self.parameters() # TODO - figure out which parameters to add.\n",
    "\t\tnorm = torch.norm(s @ W, p = 1) # Return the L1 norm\n",
    "\t\t# TODO -- add in the multinomial distribution.\n",
    "\n",
    "\t\t# TODO - need to calculate elementwise product.\n",
    "\t\treturn s @ W / norm\n",
    "\n",
    "\tdef _kl_divergence(self, mean, logvar):\n",
    "\t\t# see Appendix B from VAE paper:\n",
    "\t\t# Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "\t\t# https://arxiv.org/abs/1312.6114\n",
    "\t\t# 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "\t\t# BUT...\n",
    "\t\t# Code extended to handle a more informative prior\n",
    "\t\t# Referencing this derivation found here:\n",
    "\t\t# https://stanford.edu/~jduchi/projects/general_notes.pdf\n",
    "\t\t# Assume diagonal matrices for variance\n",
    "\t\tKLD = -0.5 * torch.sum(1 + logvar - (mean).pow(2) - logvar.exp())#, axis=0)\n",
    "\n",
    "\t\treturn KLD\n",
    "\n",
    "\tdef loss_function(self, recon_x, x, mu, logvar):\n",
    "\t\tKLD = self._kl_divergence(mu, logvar)\n",
    "\t\tPNLL = self.pois_nll(x, recon_x)\n",
    "\t\treturn torch.mean(PNLL + KLD)\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef reconstruct(self, X):\n",
    "\t\ts, W, mu, logvar = self.forward(X)\n",
    "\n",
    "\t\treturn s @ W\n",
    "\n",
    "\tdef fit(self, X, n_epochs=20, lr=1e-3, print_rate=10):\n",
    "\t\t\"\"\"\n",
    "\t\tFit the model to the data, X. Assume X is in count vector format as a tensor.\n",
    "\t\t\"\"\"\n",
    "\t\t# train_loader = DataLoader(X, batch_size=128)\n",
    "\t\toptimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\t\tfor epoch in range(n_epochs):\n",
    "\t\t\tepoch_train_loss = 0\n",
    "\t\t\tepoch_test_loss = 0\n",
    "\t\t\tfor batch_idx, data in enumerate(train_loader):\n",
    "\t\t\t\tself.train()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\ts, W, mu, logvar = self.forward(data)\n",
    "\t\t\t\trecon_batch = s @ W # Calculate the reconstructed matrix\n",
    "\t\t\t\tloss = self.loss_function(recon_batch, data, mu, logvar)\n",
    "\t\t\t\t# loss.backward()\n",
    "\t\t\t\tepoch_train_loss += loss.item()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tif batch_idx % print_rate == 0:\n",
    "\t\t\t\t\tprint('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "\t\t\t\t\t\tepoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "\t\t\t\t\t\t100. * batch_idx / len(train_loader),\n",
    "\t\t\t\t\t\tloss.item() / len(data)))\n",
    "\t\t\tprint('===> Epoch: {} Average Loss: {:.4f}'.format(\n",
    "\t\t\t\tepoch, epoch_train_loss / len(train_loader.dataset)\n",
    "\t\t\t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(vocab=v.get_vocab_size(), num_components=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 0 [0/8000 (0%)]\tLoss: 22.928799\n",
      "Train epoch: 0 [1280/8000 (16%)]\tLoss: 22.887665\n",
      "Train epoch: 0 [2560/8000 (32%)]\tLoss: 22.972055\n",
      "Train epoch: 0 [3840/8000 (48%)]\tLoss: 23.002399\n",
      "Train epoch: 0 [5120/8000 (63%)]\tLoss: 22.920671\n",
      "Train epoch: 0 [6400/8000 (79%)]\tLoss: 22.854172\n",
      "Train epoch: 0 [7680/8000 (95%)]\tLoss: 22.880749\n",
      "===> Epoch: 0 Average Loss: 23.0861\n",
      "Train epoch: 1 [0/8000 (0%)]\tLoss: 23.090324\n",
      "Train epoch: 1 [1280/8000 (16%)]\tLoss: 22.850243\n",
      "Train epoch: 1 [2560/8000 (32%)]\tLoss: 22.983200\n",
      "Train epoch: 1 [3840/8000 (48%)]\tLoss: 22.931385\n",
      "Train epoch: 1 [5120/8000 (63%)]\tLoss: 23.123175\n",
      "Train epoch: 1 [6400/8000 (79%)]\tLoss: 22.965710\n",
      "Train epoch: 1 [7680/8000 (95%)]\tLoss: 22.953072\n",
      "===> Epoch: 1 Average Loss: 23.1249\n",
      "Train epoch: 2 [0/8000 (0%)]\tLoss: 22.970270\n",
      "Train epoch: 2 [1280/8000 (16%)]\tLoss: 22.912218\n",
      "Train epoch: 2 [2560/8000 (32%)]\tLoss: 22.921179\n",
      "Train epoch: 2 [3840/8000 (48%)]\tLoss: 22.955044\n",
      "Train epoch: 2 [5120/8000 (63%)]\tLoss: 23.005392\n",
      "Train epoch: 2 [6400/8000 (79%)]\tLoss: 22.993795\n",
      "Train epoch: 2 [7680/8000 (95%)]\tLoss: 22.668013\n",
      "===> Epoch: 2 Average Loss: 23.1097\n",
      "Train epoch: 3 [0/8000 (0%)]\tLoss: 23.132624\n",
      "Train epoch: 3 [1280/8000 (16%)]\tLoss: 22.964714\n",
      "Train epoch: 3 [2560/8000 (32%)]\tLoss: 22.918055\n",
      "Train epoch: 3 [3840/8000 (48%)]\tLoss: 22.742496\n",
      "Train epoch: 3 [5120/8000 (63%)]\tLoss: 22.952829\n",
      "Train epoch: 3 [6400/8000 (79%)]\tLoss: 23.039272\n",
      "Train epoch: 3 [7680/8000 (95%)]\tLoss: 22.814461\n",
      "===> Epoch: 3 Average Loss: 23.0999\n",
      "Train epoch: 4 [0/8000 (0%)]\tLoss: 23.028042\n",
      "Train epoch: 4 [1280/8000 (16%)]\tLoss: 22.820089\n",
      "Train epoch: 4 [2560/8000 (32%)]\tLoss: 22.921066\n",
      "Train epoch: 4 [3840/8000 (48%)]\tLoss: 22.813745\n",
      "Train epoch: 4 [5120/8000 (63%)]\tLoss: 22.855602\n",
      "Train epoch: 4 [6400/8000 (79%)]\tLoss: 22.788774\n",
      "Train epoch: 4 [7680/8000 (95%)]\tLoss: 22.836508\n",
      "===> Epoch: 4 Average Loss: 23.1015\n",
      "Train epoch: 5 [0/8000 (0%)]\tLoss: 22.862577\n",
      "Train epoch: 5 [1280/8000 (16%)]\tLoss: 22.937136\n",
      "Train epoch: 5 [2560/8000 (32%)]\tLoss: 22.831053\n",
      "Train epoch: 5 [3840/8000 (48%)]\tLoss: 22.878458\n",
      "Train epoch: 5 [5120/8000 (63%)]\tLoss: 22.857534\n",
      "Train epoch: 5 [6400/8000 (79%)]\tLoss: 22.938549\n",
      "Train epoch: 5 [7680/8000 (95%)]\tLoss: 23.043341\n",
      "===> Epoch: 5 Average Loss: 23.0976\n",
      "Train epoch: 6 [0/8000 (0%)]\tLoss: 22.810793\n",
      "Train epoch: 6 [1280/8000 (16%)]\tLoss: 22.944267\n",
      "Train epoch: 6 [2560/8000 (32%)]\tLoss: 22.919765\n",
      "Train epoch: 6 [3840/8000 (48%)]\tLoss: 23.012877\n",
      "Train epoch: 6 [5120/8000 (63%)]\tLoss: 23.066345\n",
      "Train epoch: 6 [6400/8000 (79%)]\tLoss: 22.878954\n",
      "Train epoch: 6 [7680/8000 (95%)]\tLoss: 22.886511\n",
      "===> Epoch: 6 Average Loss: 23.0907\n",
      "Train epoch: 7 [0/8000 (0%)]\tLoss: 22.876966\n",
      "Train epoch: 7 [1280/8000 (16%)]\tLoss: 22.981443\n",
      "Train epoch: 7 [2560/8000 (32%)]\tLoss: 22.902996\n",
      "Train epoch: 7 [3840/8000 (48%)]\tLoss: 22.925030\n",
      "Train epoch: 7 [5120/8000 (63%)]\tLoss: 22.861032\n",
      "Train epoch: 7 [6400/8000 (79%)]\tLoss: 22.898386\n",
      "Train epoch: 7 [7680/8000 (95%)]\tLoss: 22.907043\n",
      "===> Epoch: 7 Average Loss: 23.0890\n",
      "Train epoch: 8 [0/8000 (0%)]\tLoss: 23.056408\n",
      "Train epoch: 8 [1280/8000 (16%)]\tLoss: 22.712189\n",
      "Train epoch: 8 [2560/8000 (32%)]\tLoss: 22.851528\n",
      "Train epoch: 8 [3840/8000 (48%)]\tLoss: 22.875500\n",
      "Train epoch: 8 [5120/8000 (63%)]\tLoss: 22.866995\n",
      "Train epoch: 8 [6400/8000 (79%)]\tLoss: 22.859917\n",
      "Train epoch: 8 [7680/8000 (95%)]\tLoss: 22.811747\n",
      "===> Epoch: 8 Average Loss: 23.0873\n",
      "Train epoch: 9 [0/8000 (0%)]\tLoss: 23.102413\n",
      "Train epoch: 9 [1280/8000 (16%)]\tLoss: 23.198433\n",
      "Train epoch: 9 [2560/8000 (32%)]\tLoss: 23.054708\n",
      "Train epoch: 9 [3840/8000 (48%)]\tLoss: 22.908203\n",
      "Train epoch: 9 [5120/8000 (63%)]\tLoss: 22.910589\n",
      "Train epoch: 9 [6400/8000 (79%)]\tLoss: 22.875988\n",
      "Train epoch: 9 [7680/8000 (95%)]\tLoss: 22.957685\n",
      "===> Epoch: 9 Average Loss: 23.1133\n",
      "Train epoch: 10 [0/8000 (0%)]\tLoss: 22.922289\n",
      "Train epoch: 10 [1280/8000 (16%)]\tLoss: 22.842760\n",
      "Train epoch: 10 [2560/8000 (32%)]\tLoss: 22.891830\n",
      "Train epoch: 10 [3840/8000 (48%)]\tLoss: 22.933716\n",
      "Train epoch: 10 [5120/8000 (63%)]\tLoss: 22.914568\n",
      "Train epoch: 10 [6400/8000 (79%)]\tLoss: 22.825996\n",
      "Train epoch: 10 [7680/8000 (95%)]\tLoss: 22.812006\n",
      "===> Epoch: 10 Average Loss: 23.1008\n",
      "Train epoch: 11 [0/8000 (0%)]\tLoss: 22.839844\n",
      "Train epoch: 11 [1280/8000 (16%)]\tLoss: 22.753651\n",
      "Train epoch: 11 [2560/8000 (32%)]\tLoss: 22.903427\n",
      "Train epoch: 11 [3840/8000 (48%)]\tLoss: 22.922827\n",
      "Train epoch: 11 [5120/8000 (63%)]\tLoss: 23.013683\n",
      "Train epoch: 11 [6400/8000 (79%)]\tLoss: 23.197380\n",
      "Train epoch: 11 [7680/8000 (95%)]\tLoss: 22.953556\n",
      "===> Epoch: 11 Average Loss: 23.1157\n",
      "Train epoch: 12 [0/8000 (0%)]\tLoss: 22.931602\n",
      "Train epoch: 12 [1280/8000 (16%)]\tLoss: 22.943859\n",
      "Train epoch: 12 [2560/8000 (32%)]\tLoss: 22.960556\n",
      "Train epoch: 12 [3840/8000 (48%)]\tLoss: 22.987057\n",
      "Train epoch: 12 [5120/8000 (63%)]\tLoss: 22.877277\n",
      "Train epoch: 12 [6400/8000 (79%)]\tLoss: 22.976280\n",
      "Train epoch: 12 [7680/8000 (95%)]\tLoss: 22.913101\n",
      "===> Epoch: 12 Average Loss: 23.1097\n",
      "Train epoch: 13 [0/8000 (0%)]\tLoss: 22.899916\n",
      "Train epoch: 13 [1280/8000 (16%)]\tLoss: 23.063105\n",
      "Train epoch: 13 [2560/8000 (32%)]\tLoss: 23.099432\n",
      "Train epoch: 13 [3840/8000 (48%)]\tLoss: 22.980913\n",
      "Train epoch: 13 [5120/8000 (63%)]\tLoss: 22.926247\n",
      "Train epoch: 13 [6400/8000 (79%)]\tLoss: 22.742804\n",
      "Train epoch: 13 [7680/8000 (95%)]\tLoss: 22.835625\n",
      "===> Epoch: 13 Average Loss: 23.1082\n",
      "Train epoch: 14 [0/8000 (0%)]\tLoss: 22.847330\n",
      "Train epoch: 14 [1280/8000 (16%)]\tLoss: 22.854565\n",
      "Train epoch: 14 [2560/8000 (32%)]\tLoss: 23.014946\n",
      "Train epoch: 14 [3840/8000 (48%)]\tLoss: 22.982491\n",
      "Train epoch: 14 [5120/8000 (63%)]\tLoss: 22.888847\n",
      "Train epoch: 14 [6400/8000 (79%)]\tLoss: 23.088333\n",
      "Train epoch: 14 [7680/8000 (95%)]\tLoss: 22.911051\n",
      "===> Epoch: 14 Average Loss: 23.1116\n",
      "Train epoch: 15 [0/8000 (0%)]\tLoss: 22.879309\n",
      "Train epoch: 15 [1280/8000 (16%)]\tLoss: 22.892900\n",
      "Train epoch: 15 [2560/8000 (32%)]\tLoss: 22.935829\n",
      "Train epoch: 15 [3840/8000 (48%)]\tLoss: 22.864565\n",
      "Train epoch: 15 [5120/8000 (63%)]\tLoss: 23.028988\n",
      "Train epoch: 15 [6400/8000 (79%)]\tLoss: 22.841072\n",
      "Train epoch: 15 [7680/8000 (95%)]\tLoss: 22.828321\n",
      "===> Epoch: 15 Average Loss: 23.1021\n",
      "Train epoch: 16 [0/8000 (0%)]\tLoss: 22.912359\n",
      "Train epoch: 16 [1280/8000 (16%)]\tLoss: 22.805115\n",
      "Train epoch: 16 [2560/8000 (32%)]\tLoss: 22.866014\n",
      "Train epoch: 16 [3840/8000 (48%)]\tLoss: 22.886822\n",
      "Train epoch: 16 [5120/8000 (63%)]\tLoss: 22.795183\n",
      "Train epoch: 16 [6400/8000 (79%)]\tLoss: 22.824539\n",
      "Train epoch: 16 [7680/8000 (95%)]\tLoss: 22.922422\n",
      "===> Epoch: 16 Average Loss: 23.0721\n",
      "Train epoch: 17 [0/8000 (0%)]\tLoss: 23.100815\n",
      "Train epoch: 17 [1280/8000 (16%)]\tLoss: 22.922337\n",
      "Train epoch: 17 [2560/8000 (32%)]\tLoss: 22.964151\n",
      "Train epoch: 17 [3840/8000 (48%)]\tLoss: 22.976931\n",
      "Train epoch: 17 [5120/8000 (63%)]\tLoss: 22.922642\n",
      "Train epoch: 17 [6400/8000 (79%)]\tLoss: 22.800852\n",
      "Train epoch: 17 [7680/8000 (95%)]\tLoss: 23.028032\n",
      "===> Epoch: 17 Average Loss: 23.0934\n",
      "Train epoch: 18 [0/8000 (0%)]\tLoss: 22.959927\n",
      "Train epoch: 18 [1280/8000 (16%)]\tLoss: 22.980335\n",
      "Train epoch: 18 [2560/8000 (32%)]\tLoss: 23.106833\n",
      "Train epoch: 18 [3840/8000 (48%)]\tLoss: 22.818151\n",
      "Train epoch: 18 [5120/8000 (63%)]\tLoss: 23.027821\n",
      "Train epoch: 18 [6400/8000 (79%)]\tLoss: 22.975899\n",
      "Train epoch: 18 [7680/8000 (95%)]\tLoss: 22.954088\n",
      "===> Epoch: 18 Average Loss: 23.0904\n",
      "Train epoch: 19 [0/8000 (0%)]\tLoss: 22.963839\n",
      "Train epoch: 19 [1280/8000 (16%)]\tLoss: 23.037573\n",
      "Train epoch: 19 [2560/8000 (32%)]\tLoss: 22.867895\n",
      "Train epoch: 19 [3840/8000 (48%)]\tLoss: 22.962053\n",
      "Train epoch: 19 [5120/8000 (63%)]\tLoss: 22.844419\n",
      "Train epoch: 19 [6400/8000 (79%)]\tLoss: 22.933510\n",
      "Train epoch: 19 [7680/8000 (95%)]\tLoss: 22.924623\n",
      "===> Epoch: 19 Average Loss: 23.1101\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_loader, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[155.2921, 156.8593, 154.7162,  ..., 150.3946, 156.2766, 152.8408],\n",
       "        [159.8372, 161.2811, 157.3672,  ..., 159.7819, 159.5841, 160.2900],\n",
       "        [155.6090, 158.1974, 153.2449,  ..., 154.0242, 154.4203, 153.4816],\n",
       "        ...,\n",
       "        [148.8502, 147.7955, 147.4792,  ..., 146.8992, 147.4423, 147.2446],\n",
       "        [155.6352, 157.2970, 154.6425,  ..., 157.7994, 155.2149, 156.3728],\n",
       "        [149.6803, 150.9980, 148.7019,  ..., 149.6415, 149.3845, 147.7869]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reconstruct(torch.Tensor(v.X_train.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(v.X_test.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1841)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 20])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5121.4717, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(1+logvar - torch.log(torch.Tensor(1))- 1*((0-mu).pow(2)-logvar.exp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 20])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 20])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 176])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(torch.Tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "\tmodel.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>geo</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>968999128641323008</td>\n",
       "      <td>25624940</td>\n",
       "      <td>@Volker_E Am I hallucinating that you are walk...</td>\n",
       "      <td>{'place_id': '5a110d312052166f'}</td>\n",
       "      <td>2018-02-28 23:59:30</td>\n",
       "      <td>37.708075</td>\n",
       "      <td>-122.514926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>968999100757680128</td>\n",
       "      <td>8888</td>\n",
       "      <td>There‚Äôs just something weird about living in N...</td>\n",
       "      <td>{'place_id': '5ef5b7f391e30aff'}</td>\n",
       "      <td>2018-02-28 23:59:23</td>\n",
       "      <td>37.845953</td>\n",
       "      <td>-122.324818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>968999065273774080</td>\n",
       "      <td>1557223812</td>\n",
       "      <td>Tomorrow‚Äôs March sheeeesh I‚Äôm basically 24 üòï t...</td>\n",
       "      <td>{'place_id': '5ecbd073f39c00fa'}</td>\n",
       "      <td>2018-02-28 23:59:14</td>\n",
       "      <td>37.592632</td>\n",
       "      <td>-122.160814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>968999056537088000</td>\n",
       "      <td>1164993320</td>\n",
       "      <td>Wednesday really be draining 6-6 school day üò¥</td>\n",
       "      <td>{'place_id': '5ecbd073f39c00fa'}</td>\n",
       "      <td>2018-02-28 23:59:12</td>\n",
       "      <td>37.592632</td>\n",
       "      <td>-122.160814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>968999038493245440</td>\n",
       "      <td>18650764</td>\n",
       "      <td>@remedy415 @Brycesavoy510 This hella dope</td>\n",
       "      <td>{'place_id': 'ab2f2fac83aa388d'}</td>\n",
       "      <td>2018-02-28 23:59:08</td>\n",
       "      <td>37.699279</td>\n",
       "      <td>-122.342660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206690</th>\n",
       "      <td>958852576660738048</td>\n",
       "      <td>45073046</td>\n",
       "      <td>Which one of you flatfoots stole Red Panda‚Äôs u...</td>\n",
       "      <td>{'place_id': '1a5fd1b93128bb9e'}</td>\n",
       "      <td>2018-02-01 00:00:43</td>\n",
       "      <td>37.678709</td>\n",
       "      <td>-122.130814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206691</th>\n",
       "      <td>958852515361046529</td>\n",
       "      <td>315133994</td>\n",
       "      <td>@1113JD @Nikkiii_88 @CoryBooker Him! Nah! But ...</td>\n",
       "      <td>{'place_id': '5ef5b7f391e30aff'}</td>\n",
       "      <td>2018-02-01 00:00:28</td>\n",
       "      <td>37.845953</td>\n",
       "      <td>-122.324818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206692</th>\n",
       "      <td>958852485124317184</td>\n",
       "      <td>559198723</td>\n",
       "      <td>Trying not to cry out of pain during my 3 hour...</td>\n",
       "      <td>{'place_id': '5a110d312052166f'}</td>\n",
       "      <td>2018-02-01 00:00:21</td>\n",
       "      <td>37.708075</td>\n",
       "      <td>-122.514926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206693</th>\n",
       "      <td>958852474856554496</td>\n",
       "      <td>33448971</td>\n",
       "      <td>@takkubun üòä Thank you again ‚ù§Ô∏è</td>\n",
       "      <td>{'place_id': '99e789320196ef6a'}</td>\n",
       "      <td>2018-02-01 00:00:19</td>\n",
       "      <td>37.827015</td>\n",
       "      <td>-122.315509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206694</th>\n",
       "      <td>958852442824691712</td>\n",
       "      <td>242415994</td>\n",
       "      <td>@caralmberg I knew it. üòÇ</td>\n",
       "      <td>{'place_id': '68e99cfd0ac73fb1'}</td>\n",
       "      <td>2018-02-01 00:00:11</td>\n",
       "      <td>37.877292</td>\n",
       "      <td>-122.523959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206695 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id   author_id  \\\n",
       "0       968999128641323008    25624940   \n",
       "1       968999100757680128        8888   \n",
       "2       968999065273774080  1557223812   \n",
       "3       968999056537088000  1164993320   \n",
       "4       968999038493245440    18650764   \n",
       "...                    ...         ...   \n",
       "206690  958852576660738048    45073046   \n",
       "206691  958852515361046529   315133994   \n",
       "206692  958852485124317184   559198723   \n",
       "206693  958852474856554496    33448971   \n",
       "206694  958852442824691712   242415994   \n",
       "\n",
       "                                                     text  \\\n",
       "0       @Volker_E Am I hallucinating that you are walk...   \n",
       "1       There‚Äôs just something weird about living in N...   \n",
       "2       Tomorrow‚Äôs March sheeeesh I‚Äôm basically 24 üòï t...   \n",
       "3           Wednesday really be draining 6-6 school day üò¥   \n",
       "4               @remedy415 @Brycesavoy510 This hella dope   \n",
       "...                                                   ...   \n",
       "206690  Which one of you flatfoots stole Red Panda‚Äôs u...   \n",
       "206691  @1113JD @Nikkiii_88 @CoryBooker Him! Nah! But ...   \n",
       "206692  Trying not to cry out of pain during my 3 hour...   \n",
       "206693                     @takkubun üòä Thank you again ‚ù§Ô∏è   \n",
       "206694                           @caralmberg I knew it. üòÇ   \n",
       "\n",
       "                                     geo           created_at        lat  \\\n",
       "0       {'place_id': '5a110d312052166f'}  2018-02-28 23:59:30  37.708075   \n",
       "1       {'place_id': '5ef5b7f391e30aff'}  2018-02-28 23:59:23  37.845953   \n",
       "2       {'place_id': '5ecbd073f39c00fa'}  2018-02-28 23:59:14  37.592632   \n",
       "3       {'place_id': '5ecbd073f39c00fa'}  2018-02-28 23:59:12  37.592632   \n",
       "4       {'place_id': 'ab2f2fac83aa388d'}  2018-02-28 23:59:08  37.699279   \n",
       "...                                  ...                  ...        ...   \n",
       "206690  {'place_id': '1a5fd1b93128bb9e'}  2018-02-01 00:00:43  37.678709   \n",
       "206691  {'place_id': '5ef5b7f391e30aff'}  2018-02-01 00:00:28  37.845953   \n",
       "206692  {'place_id': '5a110d312052166f'}  2018-02-01 00:00:21  37.708075   \n",
       "206693  {'place_id': '99e789320196ef6a'}  2018-02-01 00:00:19  37.827015   \n",
       "206694  {'place_id': '68e99cfd0ac73fb1'}  2018-02-01 00:00:11  37.877292   \n",
       "\n",
       "               lon  \n",
       "0      -122.514926  \n",
       "1      -122.324818  \n",
       "2      -122.160814  \n",
       "3      -122.160814  \n",
       "4      -122.342660  \n",
       "...            ...  \n",
       "206690 -122.130814  \n",
       "206691 -122.324818  \n",
       "206692 -122.514926  \n",
       "206693 -122.315509  \n",
       "206694 -122.523959  \n",
       "\n",
       "[206695 rows x 7 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = '../data/san_francisco/2018-02.csv'\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TweetsDataset(Dataset):\n",
    "\t\"\"\"Tweet Dataset\"\"\"\n",
    "\tdef __init__(self, path, agg_count=1000):\n",
    "\t\t\"\"\"\n",
    "\t\tInput:\n",
    "\t\t\tpath: file name of preprocessed count vector JSON.\n",
    "\t\t\tagg_count: the number of tweets to aggregate by.\n",
    "\t\t\tsample_rate: the number of total samples that we want to get\n",
    "\t\t\"\"\"\n",
    "\t\tself.path = path\n",
    "\t\tself.agg_count = agg_count\n",
    "\t\tself.sample_rate = sample_rate\n",
    "\t\tself.data = pass # TODO - read JSON\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(data.dates)*self.sample_rate\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\n",
    "\t\t# Randomly sample a date\n",
    "\n",
    "\t\t# Randomly sample agg_count tweets and agg\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_ex = list({1,2,3,4,5,6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_ex[20%5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out functionality\n",
    "df = pd.read_csv(\"../data/san_francisco/2018-02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dates = [datetime.strptime(d,'%Y-%m-%d %H:%M:%S').date() for d in\n",
    "            df['created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(set(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2018, 2, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = df[df['date'] == d]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@Crypto__Honey @marcan42 @siavashg The ENTIRE POINT of blockchains is that adding a transaction is an arms race, that it can‚Äôt be hijacked by a single party amassing more computing power. If it were efficient to solve, the Russians or Mafia would hijack it and it‚Äôd collapse. Inefficiency is MANDATORY',\n",
       "       'we really got a hustle player in the all star game again lmaooooooooo',\n",
       "       '@callaghannz @SaaStrAnnual Great insight being shared here @saastr',\n",
       "       'My eyelid keeps twitchingüôÑ', 'Bet u said Wtf'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.sample(5).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.DataFrame({\n",
    "    \"col\":[np.array([1,1]),np.array([2,2]),np.array([3,3]),np.array([4,4])]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t['col'].sum()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
